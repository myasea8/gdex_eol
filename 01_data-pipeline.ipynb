{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4120de72-3ee0-45d8-899a-fb40644adec2",
   "metadata": {},
   "source": [
    "# M2HATS Field Campaign Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139af74-e5eb-4d20-99a3-1499618b21e9",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "Use GDEX to read, standardize, and convert files relevant to the M2HATS field campaign to Zarr for more efficient data processing.\n",
    "\n",
    "### Data\n",
    "The two datasets used in this example are ERA5 reanalysis on pressure levels (stored on GDEX) and 30-minute 449MHz Wind Profiler data (from EOL's Field Data Archive and temporarily stored on GDEX). \n",
    "\n",
    "### Motivation\n",
    "The old data comparison process involved numerous manual steps: downloading ERA5 reanalysis data from the Copernicus Climate Data Store, and manually downloading Wind Profiler data from EOL's FDA, then untarring and unzipping the dataset. The intent for this new process is to limit the number of steps required to perform analysis and use data formats more compatible with Python's processing tools.\n",
    "\n",
    "### Audience\n",
    "Any researcher or PI interested in performing analyses using EOL's in-situ data, who is looking to modernize their workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8457e83-f17a-4f4f-9d65-683e5c71728c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909ec29-a2e6-4cd0-b2af-79c9cd00d84c",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0c47d-337c-418e-81ec-4b2e82215aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis code\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# For Dask + cluster\n",
    "from dask_jobqueue import PBSCluster\n",
    "from distributed import Client\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f18d1-1a26-408e-8bd8-19668c7bc864",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cbf96-5495-4b44-a96f-df1087e83a25",
   "metadata": {},
   "source": [
    "## Designate a scratch directory\n",
    "Define the designated scratch directory to hold Zarr stores created from field campaign data and ERA5 model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df413ab4-705b-46c0-bd2b-a3b5cf0115fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lustre_scratch  = \"/lustre/desc1/scratch/myasears\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d26f0-123e-4285-9c2b-dca51da096e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa1772-7364-46cf-a9db-a293c1fd5e69",
   "metadata": {},
   "source": [
    "## Spin up a cluster\n",
    "Create a cluster and scale it to 5 workers to assist with the processing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0888a-299e-4e28-afed-2d90c6bb2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = PBSCluster(\n",
    "        job_name = 'dask-eol-25',\n",
    "        cores = 1,\n",
    "        memory = '4GiB',\n",
    "        processes = 1,\n",
    "        local_directory = lustre_scratch + '/dask/spill',\n",
    "        log_directory = lustre_scratch + '/dask/logs/',\n",
    "        resource_spec = 'select=1:ncpus=1:mem=4GB',\n",
    "        queue = 'casper',\n",
    "        walltime = '3:00:00',\n",
    "        interface = 'ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b736d5-805c-47f3-a6b1-7c3458fdfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ef87b-0c1f-4628-bd4b-9dd438977c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 5\n",
    "cluster.scale(n_workers)\n",
    "client.wait_for_workers(n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb35e9a-bb93-4563-ad93-0afb5473c518",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e03e1-a4e5-4907-813e-f4c0aaf062bf",
   "metadata": {},
   "source": [
    "## Load 449 data\n",
    "This dataset was initially downloaded from EOL's Field Data Archive (FDA), then placed into a directory on the GDEX designated for this pilot study. The 449 MHz profiler dataset is stored in daily netcdf files, wherein each data variable depends on time (every 30 minutes) and height (every 100 meters). \n",
    "\n",
    "We would typically open this grouping of netcdf files using xarray's `open_mfdataset`, but each day of profiler data has a slightly different maximum height dimension, which requires a special process to align the height dimension before concatenating the datasets. Once these issues are resolved and the dataset is standardized for enhanced understanding and workflow, the concatenated dataset is converted to a zarr store for ease of future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c1627-692c-4566-bbd7-add87ae82148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profiler file path and create a list of all netcdf files\n",
    "prof449_path = \"/gdex/data/special_projects/pythia_2025/eol-cookbook/m2hats_iss2_data/prof449Mhz_30min_winds\"\n",
    "files = sorted(glob.glob(f\"{prof449_path}/*.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c34b37-7885-4640-afda-d2ea04885b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All height values begin with a multiple of 100 and increase by 100, but have different max and min values.\n",
    "# This code determines the global min and max of the entire field campaign's profiler data.  \n",
    "def get_minmax_alt(f):\n",
    "    with xr.open_dataset(f, decode_cf=False) as tmp:\n",
    "        return float(tmp['height'].min()), float(tmp['height'].max())\n",
    "        \n",
    "min_heights, max_heights = zip(*[get_minmax_alt(f) for f in files])\n",
    "min_height, max_height = min(min_heights), max(max_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874941b1-4540-4303-8a2c-f040eff38536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create common agl and msl height grids from the min/max height values.\n",
    "# Altitude is taken from the fifth file to avoid the instrument's set up period (manually checked).\n",
    "common_agl = np.arange(min_height, max_height + 100, 100)\n",
    "common_msl = common_agl + xr.open_dataset(files[5]).alt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f01bd2-faa0-40bf-a9ee-5b9fad629d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_regrid(f, common_agl, common_msl):\n",
    "    \"\"\"\n",
    "    Open a dataset and regrid its height coordinates to common AGL/MSL grids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open dataset with dask-style lazy loading\n",
    "    ds = xr.open_dataset(f, chunks=\"auto\")\n",
    "\n",
    "    # Make height coordinate 1-dimensional\n",
    "    height_1d = ds['height'].isel(time=0).values\n",
    "    ds = ds.assign_coords(height=(\"height\", height_1d))\n",
    "\n",
    "    # Reindex height coords to span min + max from entire campaign\n",
    "    ds = ds.reindex(height=common_agl)\n",
    "\n",
    "    # Add corresponding AGL and MSL coordinates\n",
    "    ds = ds.assign_coords(\n",
    "        height_agl=(\"height\", common_agl),\n",
    "        height_msl=(\"height\", common_msl)\n",
    "    )\n",
    "\n",
    "    # Swap to MSL as the primary height dimension\n",
    "    ds = ds.swap_dims({\"height\": \"height_msl\"}).drop_vars(\"height\")\n",
    "    ds.height_msl.attrs.update({\"long_name\": \"Height above mean sea level\", \"units\": \"m\"})\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485598f8-f5bb-41fe-8dca-75e7462cac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and regrid all datasets, then concatenate into a single 449 MHz profiler dataset\n",
    "datasets = [delayed(open_and_regrid)(f, common_agl, common_msl) for f in files[2:]]\n",
    "datasets = [d.compute() for d in datasets]\n",
    "combined_profiler = xr.concat(datasets, dim=\"time\", combine_attrs=\"override\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8d0e2-21c3-44e7-bd06-e9548400305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse lat/lon/alt variables to a single value and assign them as coordinates.\n",
    "combined_profiler = combined_profiler.assign_coords(\n",
    "    latitude=combined_profiler[\"lat\"].isel(time=0).item(),\n",
    "    longitude=combined_profiler[\"lon\"].isel(time=0).item(),\n",
    "    altitude=combined_profiler[\"alt\"].isel(time=0).item()\n",
    ").drop_vars([\"lat\", \"lon\", \"alt\"])\n",
    "\n",
    "# Standardize the naming conventions\n",
    "name_mapping = {\n",
    "    \"u\": \"u_wind\",\n",
    "    \"v\": \"v_wind\",\n",
    "    \"wvert\": \"w_wind\"\n",
    "}\n",
    "combined_profiler = combined_profiler.rename(name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb89ce2-a352-47a5-836b-64d9c57cba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the profiler data to a zarr store\n",
    "combined_profiler = combined_profiler.chunk({\"time\": 48, \"height_msl\": -1})\n",
    "combined_profiler.to_zarr(f\"{lustre_scratch}/2023_M2HATS/prof449_M2HATS_ISS1_winds30.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294e4fd-5fd0-41ce-9c0e-816eb54f47ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555603b-81fc-48c4-8a9d-46468c459e65",
   "metadata": {},
   "source": [
    "## Load ERA5 data\n",
    "The dataset for ERA5 reanalysis on pressure levels is stored on GDEX, so we bypass any necessity to download files from the CDS. ERA5 reanalysis data is stored in netcdf files separated by day and variable, wherein each data variable depends on time (every hour) and pressure (a standardized pressure grid). We would normally be able to read this data quite simply using 'intake', but this case study is unique in that we are interested in atmospheric profiles form a single lat/lon point for this analysis, and the ERA5 data are stored on pressure levels over an xy plane spanning the entire globe. \n",
    "\n",
    "To work with this information, we lazily load the all relevant monthly datasets for a single variable, then subset the Xarray Dataset by the lat/lon of the profiler and all times spanning the target field campaign. This process is repeated for all desired variables, then all resulting datasets are merged together to produce an all-inclusive dataset for the field campaign. \n",
    "\n",
    "Upon creating the concatenated dataset, we also implement code to interpolate the data variables onto a common msl height grid for direct comparison with the other ISS instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b8c70-cf00-4b45-b8de-77d6b6057600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the specifications from the M2HATS field campaign\n",
    "target_lat = 38.0\n",
    "target_lon = 243.0\n",
    "\n",
    "start_date = pd.Timestamp(\"2023-07-11T00:00:00\")\n",
    "end_date = pd.Timestamp(\"2023-09-27T23:59:59\")\n",
    "yyyymm = [\"202307\", \"202308\", \"202309\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9116c1-a366-4564-ba7a-9e1c555c44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ERA5 file path \n",
    "era5_path = '/gdex/data/d633000/e5.oper.an.pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf3d7b-9c24-46ac-a02b-cd72e5e38b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_variable(file_prefix, yyyymm):\n",
    "    \"\"\"Open, subset, and combine ERA5 files for a given variable and date range.\"\"\"\n",
    "    files = []\n",
    "    for month in yyyymm:\n",
    "        files.extend(sorted(glob.glob(f'{era5_path}/{month}/{file_prefix}*')))\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", parallel=True)\n",
    "    ds_point = ds.sel(latitude=target_lat, longitude=target_lon, time=slice(start_date, end_date))\n",
    "    \n",
    "    return ds_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda86e50-2f9f-4504-80b3-d5be15718f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the variables to be included in the final dataset\n",
    "var_map = {\"Z\": \"e5.oper.an.pl.128_129_z\",\n",
    "           \"U\": \"e5.oper.an.pl.128_131_u\",\n",
    "           \"V\": \"e5.oper.an.pl.128_132_v\",\n",
    "           \"W\": \"e5.oper.an.pl.128_135_w\"\n",
    "           }\n",
    "\n",
    "# Open, subset, and combine files from all variables\n",
    "datasets = [open_variable(file_prefix, yyyymm) for file_prefix in var_map.values()]\n",
    "\n",
    "# Merge them together into one xarray dataset\n",
    "combined_era5 = xr.merge(datasets, compat=\"override\", combine_attrs=\"override\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bd2a2-5f62-454d-9728-b3b19c4a6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert geopotential to MSL height\n",
    "combined_era5[\"height_msl\"] = (combined_era5[\"Z\"] * 6371008.7714) / (9.80665 * 6371008.7714 - combined_era5[\"Z\"])\n",
    "combined_era5.height_msl.attrs.update({\"long_name\": \"Height above mean sea level\", \"units\": \"meters\"})\n",
    "\n",
    "# Drop utc_date variable\n",
    "combined_era5 = combined_era5.drop_vars(\"utc_date\")\n",
    "\n",
    "# Change variable names to standardize with other datasets\n",
    "name_mapping = {\"level\": \"pressure\", \"Z\": \"geopotential\", \"U\": \"u_wind\", \"V\": \"v_wind\", \"W\": \"w_wind\"}\n",
    "combined_era5 = combined_era5.rename(name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91ca51-4c61-4452-9687-d569ce08e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_height_dependent(era5_data, altitude_levels):\n",
    "    \"\"\"\n",
    "    Interpolate ERA5 pressure-level data to a common height grid.\n",
    "    Adapted from: [Hamid Ali Syed](https://github.com/syedhamidali) (@syedhamidali)\n",
    "    Referenced at: https://discourse.pangeo.io/t/how-to-convert-era5-pressure-coordinates-to-altitude/4071\n",
    "    \"\"\"\n",
    "\n",
    "    interpolated_vars = {}\n",
    "    for var in era5_data.data_vars:\n",
    "        if var in [\"height_msl\", \"geopotential\"]:\n",
    "            continue\n",
    "    \n",
    "        # Interpolate along altitude with apply_ufunc\n",
    "        interp_data = xr.apply_ufunc(\n",
    "            lambda x, y: interp1d(y, x, bounds_error=False, fill_value=\"extrapolate\")(\n",
    "                altitude_levels\n",
    "            ),\n",
    "            era5_data[var],\n",
    "            era5_data[\"height_msl\"],\n",
    "            input_core_dims=[[\"pressure\"], [\"pressure\"]],\n",
    "            output_core_dims=[[\"height_msl\"]],\n",
    "            dask_gufunc_kwargs={\"output_sizes\": {\"height_msl\": len(altitude_levels)}},\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[era5_data[var].dtype],\n",
    "        )\n",
    "    \n",
    "        # Store with attributes and add to dictionary\n",
    "        interp_data.attrs = era5_data[var].attrs\n",
    "        interpolated_vars[var] = interp_data\n",
    "    \n",
    "    # Create dataset with interpolated variables and coordinates\n",
    "    coords = {\n",
    "        \"time\": era5_data.time,\n",
    "        \"height_msl\": altitude_levels\n",
    "    }\n",
    "    \n",
    "    # Initialize interpolated dataset and set correct order of coordinates and data_vars\n",
    "    ds_interpolated = xr.Dataset(interpolated_vars, coords=coords).transpose(\n",
    "        \"time\", \"height_msl\"\n",
    "    )\n",
    "    \n",
    "    # Add attributes to alt coordinate\n",
    "    ds_interpolated[\"height_msl\"].attrs = {\n",
    "        \"standard_name\": \"height_msl\",\n",
    "        \"units\": \"m\",\n",
    "        \"long_name\": \"Geometric Height (above mean sea level)\",\n",
    "        \"positive\": \"up\",\n",
    "    }\n",
    "    \n",
    "    ds_interpolated[\"longitude\"].attrs = era5_data[\"longitude\"].attrs\n",
    "    ds_interpolated[\"latitude\"].attrs = era5_data[\"latitude\"].attrs\n",
    "    ds_interpolated[\"time\"].attrs = era5_data[\"time\"].attrs\n",
    "    \n",
    "    return ds_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f44c19-5b0a-426c-a273-27e494a7dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the function for height dependence\n",
    "era5_height_levels = make_height_dependent(combined_era5, common_msl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e4d9-379e-43d4-ae0f-419f9620e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ERA5 data to a zarr store\n",
    "era5_height_levels.to_zarr(f\"{lustre_scratch}/2023_M2HATS/TEST_heights.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263d668-76ca-4fac-8933-3fcea79c28e4",
   "metadata": {},
   "source": [
    "## Open the Zarr files\n",
    "A sanity check to make sure the Zarr files have the information we'd expect, in the correct format. They look good and ready to be used in analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997c44b-a512-42af-af58-29a61790fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_test_zarr = xr.open_zarr(f\"{lustre_scratch}/2023_M2HATS/era5_M2HATS_ISS1_heights.zarr\")\n",
    "era5_test_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62525aeb-07b5-4308-9aca-ff590345c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof449Mhz_test_zarr = xr.open_zarr(f\"{lustre_scratch}/2023_M2HATS/prof449_M2HATS_ISS1_winds30.zarr\")\n",
    "prof449Mhz_test_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01125ad2-be75-4232-b8aa-92cf932abb40",
   "metadata": {},
   "source": [
    "## Notes on this workflow\n",
    "The GDEX-assisted ERA5 retrieval process highlighted in this notebook took significantly less time and storage space than the previous workflow. I look forward to using GDEX for the retrieval of other large-scale models and reanalysis products. Additionally, the Zarr storage of ERA5 data subset for the M2HATS field campaign will be immensely useful during the next stages of analysis. \n",
    "\n",
    "The 449MHz profiler ingest process was very programatically similar to my current workflow, but it's much more approachable to read in netcdf files from the GDEX, rather than go through the download/untar/unzipping process that is currently required. In addition, the ability to analyze the profiler data from a Zarr store rather than numerous individual (and height-unaligned) netcdf files is expected to significantly improve my workflow. Now, I can perform analysis after only writing one or two lines to read in both ERA5 and 449 Profiler datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025b",
   "language": "python",
   "name": "npl-2025b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
