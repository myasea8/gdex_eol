{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4120de72-3ee0-45d8-899a-fb40644adec2",
   "metadata": {},
   "source": [
    "# M2HATS Field Campaign Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139af74-e5eb-4d20-99a3-1499618b21e9",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "Use GDEX to read, standardize, and convert files relevant to the M2HATS field campaign to Zarr for more efficient data processing.\n",
    "\n",
    "### Data\n",
    "The two datasets used in this example are ERA5 reanalysis on pressure levels (stored permanently on GDEX) and 30-minute 449MHz Wind Profiler data (from EOL's Field Data Archive (FDA); stored on GDEX for this case study). \n",
    "\n",
    "### Motivation\n",
    "The old data comparison process involved numerous manual steps: programatically downloading ERA5 reanalysis data from the Copernicus Climate Data Store into local storage (which took 90+ hours per field campaign); manually downloading Wind Profiler data from EOL's FDA, then untarring and unzipping the dataset; then programatically aligning both datasets on common variables using EOL's internal server. The intent for this new process is to limit the number of steps required to perform analysis and use data formats more compatible with Python's processing tools.\n",
    "\n",
    "### Audience\n",
    "Any researcher or PI interested in performing analyses using EOL's in-situ data, who is looking to modernize their workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8457e83-f17a-4f4f-9d65-683e5c71728c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909ec29-a2e6-4cd0-b2af-79c9cd00d84c",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba0c47d-337c-418e-81ec-4b2e82215aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For analysis code\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "# For Dask + cluster\n",
    "from dask_jobqueue import PBSCluster\n",
    "from distributed import Client\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f18d1-1a26-408e-8bd8-19668c7bc864",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cbf96-5495-4b44-a96f-df1087e83a25",
   "metadata": {},
   "source": [
    "## Designate a scratch directory\n",
    "Define the designated scratch directory to hold Zarr stores created from field campaign data and ERA5 model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df413ab4-705b-46c0-bd2b-a3b5cf0115fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lustre_scratch  = \"/lustre/desc1/scratch/myasears\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d26f0-123e-4285-9c2b-dca51da096e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa1772-7364-46cf-a9db-a293c1fd5e69",
   "metadata": {},
   "source": [
    "## Spin up a cluster\n",
    "Create a cluster and scale it to 5 workers to assist with the processing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d0888a-299e-4e28-afed-2d90c6bb2abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34587 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cluster = PBSCluster(\n",
    "        job_name = 'dask-eol-25',\n",
    "        cores = 1,\n",
    "        memory = '4GiB',\n",
    "        processes = 1,\n",
    "        local_directory = lustre_scratch + '/dask/spill',\n",
    "        log_directory = lustre_scratch + '/dask/logs/',\n",
    "        resource_spec = 'select=1:ncpus=1:mem=4GB',\n",
    "        queue = 'casper',\n",
    "        walltime = '3:00:00',\n",
    "        interface = 'ext')\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429ef87b-0c1f-4628-bd4b-9dd438977c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 5\n",
    "cluster.scale(n_workers)\n",
    "client.wait_for_workers(n_workers = n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb35e9a-bb93-4563-ad93-0afb5473c518",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e03e1-a4e5-4907-813e-f4c0aaf062bf",
   "metadata": {},
   "source": [
    "## Load 449 data\n",
    "This dataset was initially downloaded from EOL's FDA, then placed into a directory on the GDEX that was designated for this pilot study. The 449 MHz profiler dataset is stored in daily netcdf files, wherein each data variable depends on time (every 30 minutes) and height (every 100 meters). \n",
    "\n",
    "We would typically open this grouping of netcdf files using xarray's `open_mfdataset`, but each day of profiler data has a slightly different maximum height dimension, which requires a special process to align the height dimension before concatenating the datasets. Once these issues are resolved and the dataset is standardized for enhanced understanding and workflow, the concatenated dataset is converted to a zarr store for ease of future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5c1627-692c-4566-bbd7-add87ae82148",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof449_path = \"/gdex/data/special_projects/pythia_2025/eol-cookbook/m2hats_iss2_data/prof449Mhz_30min_winds\"\n",
    "prof449_files = sorted(glob.glob(f\"{prof449_path}/*.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8705252-7dcc-4aa4-9da3-2190a39fb849",
   "metadata": {},
   "source": [
    "#### Align 449 heights\n",
    "1. Find the min and max height values across all 449 datasets.       \n",
    "2. Establish a common height grid that extends from the min to max height value, with a step of 100m.\n",
    "3. Open each 449 dataset and reindex its height coordinates to the common height grid.\n",
    "4. Concatenate all 449 datasets into a single xarray dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c34b37-7885-4640-afda-d2ea04885b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minmax_height(f):\n",
    "    ds = xr.open_dataset(f)\n",
    "    return float(ds['height'].min()), float(ds['height'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874941b1-4540-4303-8a2c-f040eff38536",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_heights, max_heights = zip(*[get_minmax_height(f) for f in prof449_files])\n",
    "min_height, max_height = min(min_heights), max(max_heights)\n",
    "\n",
    "common_agl = np.arange(min_height, max_height + 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f01bd2-faa0-40bf-a9ee-5b9fad629d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_regrid(f, common_agl):\n",
    "    ds = xr.open_dataset(f, chunks=\"auto\")\n",
    "    ds = ds.assign_coords(height=ds.height.isel(time=0).values)\n",
    "    ds = ds.reindex(height=common_agl)\n",
    "    ds = ds.assign_coords(\n",
    "            height_agl=(\"height\", common_agl),\n",
    "            height_msl=(\"height\", common_agl + ds.alt.values))\n",
    "    ds = ds.swap_dims({\"height\": \"height_msl\"}).drop_vars(\"height\")\n",
    "    ds.height_msl.attrs.update(long_name=\"Height above mean sea level\", units=\"m\")\n",
    "    ds.height_agl.attrs.update(long_name=\"Height above ground level\", units=\"m\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a4969c-e922-426c-bfa4-72ebd5849547",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof449_datasets = [delayed(open_and_regrid)(f, common_agl) for f in prof449_files[2:]]\n",
    "prof449_datasets = [d.compute() for d in prof449_datasets]\n",
    "combined_profiler = xr.concat(prof449_datasets, dim=\"time\", combine_attrs=\"override\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0e056-4d44-4ba6-8e8a-e91d6a20f94f",
   "metadata": {},
   "source": [
    "#### Standardize and save the 449 dataset\n",
    "1. Align the variable names and defined coordinates to match a standard format.\n",
    "2. Save the reformatted 449 data to a zarr store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c8d0e2-21c3-44e7-bd06-e9548400305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_profiler = (\n",
    "    combined_profiler\n",
    "    .assign_coords({\n",
    "        \"latitude\": combined_profiler[\"lat\"].isel(time=0).item(),\n",
    "        \"longitude\": combined_profiler[\"lon\"].isel(time=0).item(),\n",
    "        \"altitude\": combined_profiler[\"alt\"].isel(time=0).item()\n",
    "    })\n",
    "    .drop_vars([\"lat\", \"lon\", \"alt\"])\n",
    "    .rename({\"u\": \"u_wind\", \"v\": \"v_wind\", \"wvert\": \"w_wind\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb89ce2-a352-47a5-836b-64d9c57cba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/myasears/tmp/ipykernel_40617/3214795621.py:2: SerializationWarning: saving variable None with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  combined_profiler.to_zarr(f\"{lustre_scratch}/2023_M2HATS/prof449_M2HATS_winds30.zarr\")\n",
      "/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/zarr/api/asynchronous.py:229: UserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x1552c2824180>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_profiler = combined_profiler.chunk({\"time\": 48, \"height_msl\": -1})\n",
    "combined_profiler.to_zarr(f\"{lustre_scratch}/2023_M2HATS/prof449_M2HATS_winds30.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294e4fd-5fd0-41ce-9c0e-816eb54f47ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555603b-81fc-48c4-8a9d-46468c459e65",
   "metadata": {},
   "source": [
    "## Load ERA5 data\n",
    "The dataset for ERA5 reanalysis on pressure levels is stored on GDEX, so we bypass any necessity to download files from the CDS. ERA5 reanalysis data is stored in netcdf files separated by day and variable, wherein each data variable depends on time (every hour) and pressure (a standardized pressure grid). We would normally be able to read this data quite simply using 'intake', but this case study is unique in that we are interested in atmospheric profiles form a single lat/lon point for this analysis, and the ERA5 data are stored on pressure levels over an xy plane spanning the entire globe. \n",
    "\n",
    "To work with this information, we lazily load the all relevant monthly datasets for a single variable, then subset the Xarray Dataset by the lat/lon of the profiler and all times spanning the target field campaign. This process is repeated for all desired variables, then all resulting datasets are merged together to produce an all-inclusive dataset for the field campaign. \n",
    "\n",
    "Upon creating the concatenated dataset, we also implement code to interpolate the data variables onto a common msl height grid for direct comparison with the other ISS instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4356d27-834c-4ff9-831e-649f3ab657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = '/gdex/data/d633000/e5.oper.an.pl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49773716-5d09-4c2b-af1e-32023c12e494",
   "metadata": {},
   "source": [
    "#### Define project parameters\n",
    "1. Retrieve latitude, longitude, start date, and end date from the 449 information. This can be done programatically or manually, keeping in mind a grid space of 0.25ยบ.\n",
    "3. List the variables to access from GDEX and their file prefixes (referenced from the available files in era5_path).       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e7b8c70-cf00-4b45-b8de-77d6b6057600",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof449_lat = 38.0\n",
    "prof449_lon = 243.0\n",
    "\n",
    "start_date = pd.Timestamp(\"2023-07-11T00:00:00\")\n",
    "end_date = pd.Timestamp(\"2023-09-27T23:59:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01a07790-5473-4d41-b14e-451ae4e86de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_vars = {\"Z\": \"e5.oper.an.pl.128_129_z\",\n",
    "             \"U\": \"e5.oper.an.pl.128_131_u\",\n",
    "             \"V\": \"e5.oper.an.pl.128_132_v\",\n",
    "             \"W\": \"e5.oper.an.pl.128_135_w\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c50cf-4185-4abb-8078-87e824646cc2",
   "metadata": {},
   "source": [
    "#### Retrieve ERA5 files\n",
    "1. Create a dataset for each variable (Z, U, V, W), for each month in the date range.\n",
    "2. Merge these datasets into a single xarray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6e93d4c-2554-4c74-8720-dbd35b2a9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_era5(file_prefix, lat, lon, start, end):\n",
    "    \n",
    "    files = []\n",
    "    yyyymm = pd.date_range(start.normalize().replace(day=1), end, freq=\"MS\").strftime(\"%Y%m\").tolist()\n",
    "    \n",
    "    for month in yyyymm:\n",
    "        files.extend(sorted(glob.glob(f'{era5_path}/{month}/{file_prefix}*')))\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", parallel=True)\n",
    "    ds_point = ds.sel(latitude=lat, longitude=lon, time=slice(start, end))\n",
    "    \n",
    "    return ds_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bda86e50-2f9f-4504-80b3-d5be15718f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [retrieve_era5(prefix, prof449_lat, prof449_lon, start_date, end_date) for prefix in era5_vars.values()]\n",
    "combined_era5 = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da776f0-0236-4d1e-afa0-23c6da10bd4e",
   "metadata": {},
   "source": [
    "#### Standardize the dataset\n",
    "1. Calculate MSL height from geopotential and add attributes.\n",
    "2. Calculate wind speed and wind direction from u and v and add attributes.\n",
    "3. Align the variable names and defined coordinates to match a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7a46778-f47d-4272-aed4-ab8d8e3b39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_era5[\"height_msl\"] = (combined_era5[\"Z\"] * 6371008.7714) / (9.80665 * 6371008.7714 - combined_era5[\"Z\"])\n",
    "combined_era5[\"height_msl\"].attrs = {\n",
    "    \"long_name\": \"Height above mean sea level\",\n",
    "    \"short_name\": \"height_msl\",\n",
    "    \"units\": \"meters\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58fb415c-e607-4a1a-be87-e13b79d31020",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = combined_era5[\"U\"].data\n",
    "v = combined_era5[\"V\"].data\n",
    "\n",
    "wspd = np.sqrt(u**2 + v**2)\n",
    "wdir = (np.degrees(np.arctan2(-u, -v)) + 360) % 360\n",
    "\n",
    "combined_era5[\"wspd\"] = ((\"time\", \"pressure\"), wspd)\n",
    "combined_era5[\"wdir\"] = ((\"time\", \"pressure\"), wdir)\n",
    "\n",
    "combined_era5[\"wspd\"].attrs = {\n",
    "    \"long_name\": \"Wind Speed\",\n",
    "    \"short_name\": \"wspd\",\n",
    "    \"units\": \"meters/second\",\n",
    "    \"source\": \"ERA5 atmospheric pressure level analysis [netCDF4] u and v wind components\",\n",
    "    \"calculation_method\": \"MetPy 1.7.0 -- metpy.calc.wind_speed(u, v)\"\n",
    "}\n",
    "\n",
    "combined_era5[\"wdir\"].attrs = {\n",
    "    \"long_name\": \"Wind Direction (from direction)\",\n",
    "    \"short_name\": \"wdir\",\n",
    "    \"units\": \"degrees (east of north)\",\n",
    "    \"source\": \"ERA5 atmospheric pressure level analysis [netCDF4] u and v wind components\",\n",
    "    \"calculation_method\": \"Numpy 1.26.4 -- arctan2(u_wind, v_wind)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf4493-12f6-4a63-b6aa-fcc47e871ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_era5 = (\n",
    "    combined_era5\n",
    "    .drop_vars(\"utc_date\")\n",
    "    .rename({\n",
    "        \"level\": \"pressure\",\n",
    "        \"Z\": \"geopotential\",\n",
    "        \"U\": \"u_wind\",\n",
    "        \"V\": \"v_wind\",\n",
    "        \"W\": \"w_wind\",\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d08221-69ad-4711-8007-ac30c5c4e85b",
   "metadata": {},
   "source": [
    "#### Make ERA5 height dependent\n",
    "For each variable:\n",
    "1. Interpolate the data along the common MSL grid used by the 449.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91ca51-4c61-4452-9687-d569ce08e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_height_dependent(era5_data, altitude_levels):\n",
    "    \"\"\"\n",
    "    Interpolate ERA5 pressure-level data to a common height grid.\n",
    "    Adapted from: [Hamid Ali Syed](https://github.com/syedhamidali) (@syedhamidali)\n",
    "    Referenced at: https://discourse.pangeo.io/t/how-to-convert-era5-pressure-coordinates-to-altitude/4071\n",
    "    \"\"\"\n",
    "\n",
    "    interpolated_vars = {}\n",
    "    for var in era5_data.data_vars:\n",
    "        if var in [\"height_msl\", \"geopotential\"]:\n",
    "            continue\n",
    "    \n",
    "        # Interpolate along altitude with apply_ufunc\n",
    "        interp_data = xr.apply_ufunc(\n",
    "            lambda x, y: interp1d(y, x, bounds_error=False, fill_value=\"extrapolate\")(\n",
    "                altitude_levels\n",
    "            ),\n",
    "            era5_data[var],\n",
    "            era5_data[\"height_msl\"],\n",
    "            input_core_dims=[[\"pressure\"], [\"pressure\"]],\n",
    "            output_core_dims=[[\"height_msl\"]],\n",
    "            dask_gufunc_kwargs={\"output_sizes\": {\"height_msl\": len(altitude_levels)}},\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[era5_data[var].dtype],\n",
    "        )\n",
    "    \n",
    "        # Store with attributes and add to dictionary\n",
    "        interp_data.attrs = era5_data[var].attrs\n",
    "        interpolated_vars[var] = interp_data\n",
    "    \n",
    "    # Create dataset with interpolated variables and coordinates\n",
    "    coords = {\n",
    "        \"time\": era5_data.time,\n",
    "        \"height_msl\": altitude_levels\n",
    "    }\n",
    "    \n",
    "    # Initialize interpolated dataset and set correct order of coordinates and data_vars\n",
    "    ds_interpolated = xr.Dataset(interpolated_vars, coords=coords).transpose(\n",
    "        \"time\", \"height_msl\"\n",
    "    )\n",
    "    \n",
    "    # Add attributes to alt coordinate\n",
    "    ds_interpolated[\"height_msl\"].attrs = {\n",
    "        \"standard_name\": \"height_msl\",\n",
    "        \"units\": \"m\",\n",
    "        \"long_name\": \"Geometric Height (above mean sea level)\",\n",
    "        \"positive\": \"up\",\n",
    "    }\n",
    "    \n",
    "    ds_interpolated[\"longitude\"].attrs = era5_data[\"longitude\"].attrs\n",
    "    ds_interpolated[\"latitude\"].attrs = era5_data[\"latitude\"].attrs\n",
    "    ds_interpolated[\"time\"].attrs = era5_data[\"time\"].attrs\n",
    "    \n",
    "    return ds_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f44c19-5b0a-426c-a273-27e494a7dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_height_levels = make_height_dependent(combined_era5, common_msl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b809f-c633-4e6c-95ca-d531795d636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_height_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009dfb5-9186-408a-8874-b3112e6733b6",
   "metadata": {},
   "source": [
    "#### Save ERA5 as a zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e4d9-379e-43d4-ae0f-419f9620e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_height_levels.to_zarr(f\"{lustre_scratch}/2023_M2HATS/TEST_heights.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263d668-76ca-4fac-8933-3fcea79c28e4",
   "metadata": {},
   "source": [
    "## Open the Zarr files\n",
    "A sanity check to make sure the Zarr files have the information we'd expect, in the correct format. They look good and ready to be used in analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997c44b-a512-42af-af58-29a61790fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_test_zarr = xr.open_zarr(f\"{lustre_scratch}/2023_M2HATS/era5_M2HATS_ISS1_heights.zarr\")\n",
    "era5_test_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62525aeb-07b5-4308-9aca-ff590345c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof449Mhz_test_zarr = xr.open_zarr(f\"{lustre_scratch}/2023_M2HATS/prof449_M2HATS_ISS1_winds30.zarr\")\n",
    "prof449Mhz_test_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01125ad2-be75-4232-b8aa-92cf932abb40",
   "metadata": {},
   "source": [
    "## Notes on this workflow\n",
    "The GDEX-assisted ERA5 retrieval process highlighted in this notebook took significantly less time and storage space than the previous workflow. I look forward to using GDEX for the retrieval of other large-scale models and reanalysis products. Additionally, the Zarr storage of ERA5 data subset for the M2HATS field campaign will be immensely useful during the next stages of analysis. \n",
    "\n",
    "The 449MHz profiler ingest process was very programatically similar to my current workflow, but it's much more approachable to read in netcdf files from the GDEX, rather than go through the download/untar/unzipping process that is currently required. In addition, the ability to analyze the profiler data from a Zarr store rather than numerous individual (and height-unaligned) netcdf files is expected to significantly improve my workflow. Now, I can perform analysis after only writing one or two lines to read in both ERA5 and 449 Profiler datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025b",
   "language": "python",
   "name": "npl-2025b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
